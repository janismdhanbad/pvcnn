{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import plyfile\n",
    "from matplotlib import cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_label(data_dir, output_dir):\n",
    "    object_dict = {\n",
    "        'clutter': 0,\n",
    "        'ceiling': 1,\n",
    "        'floor': 2,\n",
    "        'wall': 3,\n",
    "        'beam': 4,\n",
    "        'column': 5,\n",
    "        'door': 6,\n",
    "        'window': 7,\n",
    "        'table': 8,\n",
    "        'chair': 9,\n",
    "        'sofa': 10,\n",
    "        'bookcase': 11,\n",
    "        'board': 12\n",
    "    }\n",
    "\n",
    "    for area in os.listdir(data_dir):\n",
    "        path_area = os.path.join(data_dir, area)\n",
    "        if not os.path.isdir(path_area):\n",
    "            continue\n",
    "\n",
    "        path_dir_rooms = os.listdir(path_area)\n",
    "        for room in path_dir_rooms:\n",
    "            path_annotations = os.path.join(data_dir, area, room, 'Annotations')\n",
    "            if not os.path.isdir(path_annotations):\n",
    "                continue\n",
    "\n",
    "            print(path_annotations)\n",
    "            path_prepare_label = os.path.join(output_dir, area, room)\n",
    "            if os.path.exists(os.path.join(path_prepare_label, '.labels')):\n",
    "                print(f'{path_prepare_label} already processed, skipping')\n",
    "                continue\n",
    "\n",
    "            xyz_room = np.zeros((1, 6))\n",
    "            label_room = np.zeros((1, 1))\n",
    "            # make store directories\n",
    "            if not os.path.exists(path_prepare_label):\n",
    "                os.makedirs(path_prepare_label)\n",
    "\n",
    "            path_objects = os.listdir(path_annotations)\n",
    "            for obj in path_objects:\n",
    "                object_key = obj.split('_', 1)[0]\n",
    "                try:\n",
    "                    val = object_dict[object_key]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                print(f'{room}/{obj[:-4]}')\n",
    "                xyz_object_path = os.path.join(path_annotations, obj)\n",
    "                try:\n",
    "                    xyz_object = np.loadtxt(xyz_object_path)[:, :]  # (N,6)\n",
    "                except ValueError as e:\n",
    "                    print(f'ERROR: cannot load {xyz_object_path}: {e}')\n",
    "                    continue\n",
    "                label_object = np.tile(val, (xyz_object.shape[0], 1))  # (N,1)\n",
    "                xyz_room = np.vstack((xyz_room, xyz_object))\n",
    "                label_room = np.vstack((label_room, label_object))\n",
    "\n",
    "            xyz_room = np.delete(xyz_room, [0], 0)\n",
    "            label_room = np.delete(label_room, [0], 0)\n",
    "\n",
    "            np.save(path_prepare_label + '/xyzrgb.npy', xyz_room)\n",
    "            np.save(path_prepare_label + '/label.npy', label_room)\n",
    "\n",
    "            # Marker indicating we've processed this room\n",
    "            open(os.path.join(path_prepare_label, '.labels'), 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(block_size=1.5, data_dir='C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/Stanford3dDataset_v1.2_Aligned_Version', grid_size=0.03, max_num_points=8192, output_dir='C:\\\\Users\\\\SING1761\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-fcb1fd3b-a763-46c4-86b1-e6176e78d166.json', save_ply=False)\n"
     ]
    }
   ],
   "source": [
    "    default_data_dir = 'C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/Stanford3dDataset_v1.2_Aligned_Version'\n",
    "    default_output_dir = 'C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--data', dest='data_dir', default=default_data_dir,\n",
    "                        help=f'Path to S3DIS data (default is {default_data_dir})')\n",
    "    parser.add_argument('-f', '--folder', dest='output_dir', default=default_output_dir,\n",
    "                        help=f'Folder to write labels (default is {default_output_dir})')\n",
    "    parser.add_argument('--max_num_points', '-m', help='Max point number of each sample', type=int, default=8192)\n",
    "    parser.add_argument('--block_size', '-b', help='Block size', type=float, default=1.5)\n",
    "    parser.add_argument('--grid_size', '-g', help='Grid size', type=float, default=0.03)\n",
    "    parser.add_argument('--save_ply', '-s', help='Convert .pts to .ply', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    default_data_dir = 'C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/Stanford3dDataset_v1.2_Aligned_Version'\n",
    "    default_output_dir = 'C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--data', dest='data_dir', default=default_data_dir,\n",
    "                        help=f'Path to S3DIS data (default is {default_data_dir})')\n",
    "    parser.add_argument('-f', '--folder', dest='output_dir', default=default_output_dir,\n",
    "                        help=f'Folder to write labels (default is {default_output_dir})')\n",
    "    parser.add_argument('--max_num_points', '-m', help='Max point number of each sample', type=int, default=8192)\n",
    "    parser.add_argument('--block_size', '-b', help='Block size', type=float, default=1.5)\n",
    "    parser.add_argument('--grid_size', '-g', help='Grid size', type=float, default=0.03)\n",
    "    parser.add_argument('--save_ply', '-s', help='Convert .pts to .ply', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    prepare_label(data_dir=args.data_dir, output_dir=default_output_dir)\n",
    "\n",
    "    root = default_output_dir\n",
    "    max_num_points = args.max_num_points\n",
    "\n",
    "    batch_size = 2048\n",
    "    data = np.zeros((batch_size, max_num_points, 9))\n",
    "    data_num = np.zeros(batch_size, dtype=np.int32)\n",
    "    label = np.zeros(batch_size, dtype=np.int32)\n",
    "    label_seg = np.zeros((batch_size, max_num_points), dtype=np.int32)\n",
    "    indices_split_to_full = np.zeros((batch_size, max_num_points), dtype=np.int32)\n",
    "\n",
    "    for area_idx in range(1, 7):\n",
    "        folder = os.path.join(root, 'Area_%d' % area_idx)\n",
    "        datasets = [dataset for dataset in os.listdir(folder)]\n",
    "        for dataset_idx, dataset in enumerate(datasets):\n",
    "            dataset_marker = os.path.join(folder, dataset, '.dataset')\n",
    "            if os.path.exists(dataset_marker):\n",
    "                print(f'{datetime.now()}-{folder}/{dataset} already processed, skipping')\n",
    "                continue\n",
    "            filename_data = os.path.join(folder, dataset, 'xyzrgb.npy')\n",
    "            print(f'{datetime.now()}-Loading {filename_data}...')\n",
    "            # Modified according to PointNet convensions.\n",
    "            xyzrgb = np.load(filename_data)\n",
    "            xyzrgb[:, 0:3] -= np.amin(xyzrgb, axis=0)[0:3]\n",
    "\n",
    "            filename_labels = os.path.join(folder, dataset, 'label.npy')\n",
    "            print(f'{datetime.now()}-Loading {filename_labels}...')\n",
    "            labels = np.load(filename_labels).astype(int).flatten()\n",
    "\n",
    "            xyz, rgb = np.split(xyzrgb, [3], axis=-1)\n",
    "            xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "            xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "            xyz_center = (xyz_min + xyz_max) / 2\n",
    "            xyz_center[0][-1] = xyz_min[0][-1]\n",
    "            # Remark: Don't do global alignment.\n",
    "            # xyz = xyz - xyz_center\n",
    "            rgb = rgb / 255.0\n",
    "            max_room_x = np.max(xyz[:, 0])\n",
    "            max_room_y = np.max(xyz[:, 1])\n",
    "            max_room_z = np.max(xyz[:, 2])\n",
    "\n",
    "            offsets = [('zero', 0.0), ('half', args.block_size / 2)]\n",
    "            for offset_name, offset in offsets:\n",
    "                idx_h5 = 0\n",
    "                idx = 0\n",
    "\n",
    "                print(f'{datetime.now()}-Computing block id of {xyzrgb.shape[0]} points...')\n",
    "                xyz_min = np.amin(xyz, axis=0, keepdims=True) - offset\n",
    "                xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "                block_size = (args.block_size, args.block_size, 2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "                # Note: Don't split over z axis.\n",
    "                xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "\n",
    "                print(f'{datetime.now()}-Collecting points belong to each block...')\n",
    "                blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                            return_counts=True, axis=0)\n",
    "                block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "                print(f'{datetime.now()}-{dataset} is split into {blocks.shape[0]} blocks.')\n",
    "\n",
    "                block_to_block_idx_map = dict()\n",
    "                for block_idx in range(blocks.shape[0]):\n",
    "                    block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "                    block_to_block_idx_map[(block[0], block[1])] = block_idx\n",
    "\n",
    "                # merge small blocks into one of their big neighbors\n",
    "                block_point_count_threshold = max_num_points/10\n",
    "                nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n",
    "                block_merge_count = 0\n",
    "                for block_idx in range(blocks.shape[0]):\n",
    "                    if block_point_counts[block_idx] >= block_point_count_threshold:\n",
    "                        continue\n",
    "\n",
    "                    block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "                    for x, y in nbr_block_offsets:\n",
    "                        nbr_block = (block[0] + x, block[1] + y)\n",
    "                        if nbr_block not in block_to_block_idx_map:\n",
    "                            continue\n",
    "\n",
    "                        nbr_block_idx = block_to_block_idx_map[nbr_block]\n",
    "                        if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n",
    "                            continue\n",
    "\n",
    "                        block_point_indices[nbr_block_idx] = np.concatenate(\n",
    "                            [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n",
    "                        block_point_indices[block_idx] = np.array([], dtype=np.int)\n",
    "                        block_merge_count = block_merge_count + 1\n",
    "                        break\n",
    "                print(f'{datetime.now()}-{block_merge_count} of {blocks.shape[0]} blocks are merged.')\n",
    "\n",
    "                idx_last_non_empty_block = 0\n",
    "                for block_idx in reversed(range(blocks.shape[0])):\n",
    "                    if block_point_indices[block_idx].shape[0] != 0:\n",
    "                        idx_last_non_empty_block = block_idx\n",
    "                        break\n",
    "\n",
    "                # uniformly sample each block\n",
    "                for block_idx in range(idx_last_non_empty_block + 1):\n",
    "                    point_indices = block_point_indices[block_idx]\n",
    "                    if point_indices.shape[0] == 0:\n",
    "                        continue\n",
    "                    block_points = xyz[point_indices]\n",
    "                    block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "                    xyz_grids = np.floor((block_points - block_min) / args.grid_size).astype(np.int)\n",
    "                    grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n",
    "                                                                             return_counts=True, axis=0)\n",
    "                    grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n",
    "                    grid_point_count_avg = int(np.average(grid_point_counts))\n",
    "                    point_indices_repeated = []\n",
    "                    for grid_idx in range(grids.shape[0]):\n",
    "                        point_indices_in_block = grid_point_indices[grid_idx]\n",
    "                        repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n",
    "                        if repeat_num > 1:\n",
    "                            point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n",
    "                            np.random.shuffle(point_indices_in_block)\n",
    "                            point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n",
    "                        point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n",
    "                    block_point_indices[block_idx] = np.array(point_indices_repeated)\n",
    "                    block_point_counts[block_idx] = len(point_indices_repeated)\n",
    "\n",
    "                for block_idx in range(idx_last_non_empty_block + 1):\n",
    "                    point_indices = block_point_indices[block_idx]\n",
    "                    if point_indices.shape[0] == 0:\n",
    "                        continue\n",
    "\n",
    "                    block_point_num = point_indices.shape[0]\n",
    "                    block_split_num = int(math.ceil(block_point_num * 1.0 / max_num_points))\n",
    "                    point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n",
    "                    point_nums = [point_num_avg] * block_split_num\n",
    "                    point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n",
    "                    starts = [0] + list(np.cumsum(point_nums))\n",
    "\n",
    "                    # Modified following convensions of PointNet.\n",
    "                    np.random.shuffle(point_indices)\n",
    "                    block_points = xyz[point_indices]\n",
    "                    block_rgb = rgb[point_indices]\n",
    "                    block_labels = labels[point_indices]\n",
    "                    x, y, z = np.split(block_points, (1, 2), axis=-1)\n",
    "                    norm_x = x / max_room_x\n",
    "                    norm_y = y / max_room_y\n",
    "                    norm_z = z / max_room_z\n",
    "                    \n",
    "                    minx = np.min(x)\n",
    "                    miny = np.min(y)\n",
    "                    x = x - (minx + args.block_size / 2)\n",
    "                    y = y - (miny + args.block_size / 2)\n",
    "                    \n",
    "                    block_xyzrgb = np.concatenate([x, y, z, block_rgb, norm_x, norm_y, norm_z], axis=-1)\n",
    "                    for block_split_idx in range(block_split_num):\n",
    "                        start = starts[block_split_idx]\n",
    "                        point_num = point_nums[block_split_idx]\n",
    "                        end = start + point_num\n",
    "                        idx_in_batch = idx % batch_size\n",
    "                        data[idx_in_batch, 0:point_num, ...] = block_xyzrgb[start:end, :]\n",
    "                        data_num[idx_in_batch] = point_num\n",
    "                        label[idx_in_batch] = dataset_idx  # won't be used...\n",
    "                        label_seg[idx_in_batch, 0:point_num] = block_labels[start:end]\n",
    "                        indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n",
    "\n",
    "                        if ((idx + 1) % batch_size == 0) or \\\n",
    "                                (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1):\n",
    "                            item_num = idx_in_batch + 1\n",
    "                            filename_h5 = os.path.join(folder, dataset, f'{offset_name}_{idx_h5:d}.h5')\n",
    "                            print(f'{datetime.now()}-Saving {filename_h5}...')\n",
    "\n",
    "                            file = h5py.File(filename_h5, 'w')\n",
    "                            file.create_dataset('data', data=data[0:item_num, ...])\n",
    "                            file.create_dataset('data_num', data=data_num[0:item_num, ...])\n",
    "                            file.create_dataset('label', data=label[0:item_num, ...])\n",
    "                            file.create_dataset('label_seg', data=label_seg[0:item_num, ...])\n",
    "                            file.create_dataset('indices_split_to_full', data=indices_split_to_full[0:item_num, ...])\n",
    "                            file.close()\n",
    "\n",
    "                            if args.save_ply:\n",
    "                                print(f'{datetime.now()}-Saving ply of {filename_h5}...')\n",
    "                                filepath_label_ply = os.path.join(folder, dataset, 'ply_label',\n",
    "                                                                  f'label_{offset_name}_{idx_h5:d}')\n",
    "                                save_ply_property_batch(data[0:item_num, :, 0:3], label_seg[0:item_num, ...],\n",
    "                                                        filepath_label_ply, data_num[0:item_num, ...], 14)\n",
    "\n",
    "                                filepath_rgb_ply = os.path.join(folder, dataset, 'ply_rgb',\n",
    "                                                                f'rgb_{offset_name}_{idx_h5:d}')\n",
    "                                save_ply_color_batch(data[0:item_num, :, 0:3], data[0:item_num, :, 3:6] * 255,\n",
    "                                                     filepath_rgb_ply, data_num[0:item_num, ...])\n",
    "\n",
    "                            idx_h5 = idx_h5 + 1\n",
    "                        idx = idx + 1\n",
    "\n",
    "            # Marker indicating we've processed this dataset\n",
    "            open(dataset_marker, 'w').close()\n",
    "    print(f'{datetime.now()}-Done.')\n",
    "\n",
    "\n",
    "def save_ply(points, filename, colors=None, normals=None):\n",
    "    vertex = np.core.records.fromarrays(points.transpose(), names='x, y, z', formats='f4, f4, f4')\n",
    "    n = len(vertex)\n",
    "    desc = vertex.dtype.descr\n",
    "\n",
    "    if normals is not None:\n",
    "        vertex_normal = np.core.records.fromarrays(normals.transpose(), names='nx, ny, nz', formats='f4, f4, f4')\n",
    "        assert len(vertex_normal) == n\n",
    "        desc = desc + vertex_normal.dtype.descr\n",
    "\n",
    "    if colors is not None:\n",
    "        vertex_color = np.core.records.fromarrays(colors.transpose() * 255, names='red, green, blue',\n",
    "                                                  formats='u1, u1, u1')\n",
    "        assert len(vertex_color) == n\n",
    "        desc = desc + vertex_color.dtype.descr\n",
    "\n",
    "    vertex_all = np.empty(n, dtype=desc)\n",
    "\n",
    "    for prop in vertex.dtype.names:\n",
    "        vertex_all[prop] = vertex[prop]\n",
    "\n",
    "    if normals is not None:\n",
    "        for prop in vertex_normal.dtype.names:\n",
    "            vertex_all[prop] = vertex_normal[prop]\n",
    "\n",
    "    if colors is not None:\n",
    "        for prop in vertex_color.dtype.names:\n",
    "            vertex_all[prop] = vertex_color[prop]\n",
    "\n",
    "    ply = plyfile.PlyData([plyfile.PlyElement.describe(vertex_all, 'vertex')], text=False)\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    ply.write(filename)\n",
    "\n",
    "\n",
    "def save_ply_property(points, property, property_max, filename, cmap_name='tab20'):\n",
    "    point_num = points.shape[0]\n",
    "    colors = np.full(points.shape, 0.5)\n",
    "    cmap = cm.get_cmap(cmap_name)\n",
    "    for point_idx in range(point_num):\n",
    "        if property[point_idx] == 0:\n",
    "            colors[point_idx] = np.array([0, 0, 0])\n",
    "        else:\n",
    "            colors[point_idx] = cmap(property[point_idx] / property_max)[:3]\n",
    "    save_ply(points, filename, colors)\n",
    "\n",
    "\n",
    "def save_ply_color_batch(points_batch, colors_batch, file_path, points_num=None):\n",
    "    batch_size = points_batch.shape[0]\n",
    "    if not isinstance(file_path, (list, tuple)):\n",
    "        basename = os.path.splitext(file_path)[0]\n",
    "        ext = '.ply'\n",
    "    for batch_idx in range(batch_size):\n",
    "        point_num = points_batch.shape[1] if points_num is None else points_num[batch_idx]\n",
    "        if isinstance(file_path, (list, tuple)):\n",
    "            save_ply(points_batch[batch_idx][:point_num], file_path[batch_idx], colors_batch[batch_idx][:point_num])\n",
    "        else:\n",
    "            save_ply(points_batch[batch_idx][:point_num], f'{basename}_{batch_idx:04d}{ext}',\n",
    "                     colors_batch[batch_idx][:point_num])\n",
    "\n",
    "\n",
    "def save_ply_property_batch(points_batch, property_batch, file_path, points_num=None, property_max=None,\n",
    "                            cmap_name='tab20'):\n",
    "    batch_size = points_batch.shape[0]\n",
    "    if not isinstance(file_path, (list, tuple)):\n",
    "        basename = os.path.splitext(file_path)[0]\n",
    "        ext = '.ply'\n",
    "    property_max = np.max(property_batch) if property_max is None else property_max\n",
    "    for batch_idx in range(batch_size):\n",
    "        point_num = points_batch.shape[1] if points_num is None else points_num[batch_idx]\n",
    "        if isinstance(file_path, (list, tuple)):\n",
    "            save_ply_property(points_batch[batch_idx][:point_num], property_batch[batch_idx][:point_num],\n",
    "                              property_max, file_path[batch_idx], cmap_name)\n",
    "        else:\n",
    "            save_ply_property(points_batch[batch_idx][:point_num], property_batch[batch_idx][:point_num],\n",
    "                              property_max, f'{basename}_{batch_idx:04d}{ext}', cmap_name)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(block_size=1.5, data_dir='C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/Stanford3dDataset_v1.2_Aligned_Version', grid_size=0.03, max_num_points=8192, output_dir='C:\\\\Users\\\\SING1761\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-fcb1fd3b-a763-46c4-86b1-e6176e78d166.json', save_ply=False)\n"
     ]
    }
   ],
   "source": [
    "    default_data_dir = 'C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/Stanford3dDataset_v1.2_Aligned_Version'\n",
    "    default_output_dir = 'C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--data', dest='data_dir', default=default_data_dir,\n",
    "                        help=f'Path to S3DIS data (default is {default_data_dir})')\n",
    "    parser.add_argument('-f', '--folder', dest='output_dir', default=default_output_dir,\n",
    "                        help=f'Folder to write labels (default is {default_output_dir})')\n",
    "    parser.add_argument('--max_num_points', '-m', help='Max point number of each sample', type=int, default=8192)\n",
    "    parser.add_argument('--block_size', '-b', help='Block size', type=float, default=1.5)\n",
    "    parser.add_argument('--grid_size', '-g', help='Grid size', type=float, default=0.03)\n",
    "    parser.add_argument('--save_ply', '-s', help='Convert .pts to .ply', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "    root = default_output_dir\n",
    "    max_num_points = args.max_num_points\n",
    "\n",
    "    batch_size = 2048\n",
    "    data = np.zeros((batch_size, max_num_points, 9))\n",
    "    data_num = np.zeros(batch_size, dtype=np.int32)\n",
    "    label = np.zeros(batch_size, dtype=np.int32)\n",
    "    label_seg = np.zeros((batch_size, max_num_points), dtype=np.int32)\n",
    "    indices_split_to_full = np.zeros((batch_size, max_num_points), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "        folder = os.path.join(root, 'Area_%d' % area_idx)\n",
    "        datasets = [dataset for dataset in os.listdir(folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conferenceRoom_1', 'conferenceRoom_2']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xyzrgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3981f36a1bd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxyzrgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xyzrgb' is not defined"
     ]
    }
   ],
   "source": [
    "xyzrgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 06:41:02.889438-Loading C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn\\Area_1\\conferenceRoom_1\\xyzrgb.npy...\n",
      "2020-06-09 06:41:03.072886-Loading C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn\\Area_1\\conferenceRoom_1\\label.npy...\n",
      "2020-06-09 06:41:03.206749-Computing block id of 1136617 points...\n",
      "2020-06-09 06:41:03.358850-Collecting points belong to each block...\n",
      "2020-06-09 06:41:04.907361-conferenceRoom_1 is split into 12 blocks.\n",
      "2020-06-09 06:41:04.915286-Computing block id of 1136617 points...\n",
      "2020-06-09 06:41:05.059834-Collecting points belong to each block...\n",
      "2020-06-09 06:41:06.451646-conferenceRoom_1 is split into 19 blocks.\n",
      "2020-06-09 06:41:06.451646-Loading C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn\\Area_1\\conferenceRoom_2\\xyzrgb.npy...\n",
      "2020-06-09 06:41:06.624366-Loading C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn\\Area_1\\conferenceRoom_2\\label.npy...\n",
      "2020-06-09 06:41:06.803221-Computing block id of 1535040 points...\n",
      "2020-06-09 06:41:06.999419-Collecting points belong to each block...\n",
      "2020-06-09 06:41:09.319872-conferenceRoom_2 is split into 20 blocks.\n",
      "2020-06-09 06:41:09.322087-Computing block id of 1535040 points...\n",
      "2020-06-09 06:41:09.544185-Collecting points belong to each block...\n",
      "2020-06-09 06:41:11.430493-conferenceRoom_2 is split into 25 blocks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        for dataset_idx, dataset in enumerate(datasets):\n",
    "            dataset_marker = os.path.join(folder, dataset, '.dataset')\n",
    "#             if os.path.exists(dataset_marker):\n",
    "#                 print(f'{datetime.now()}-{folder}/{dataset} already processed, skipping')\n",
    "#                 continue\n",
    "            filename_data = os.path.join(folder, dataset, 'xyzrgb.npy')\n",
    "            print(f'{datetime.now()}-Loading {filename_data}...')\n",
    "            # Modified according to PointNet convensions.\n",
    "            xyzrgb = np.load(filename_data)\n",
    "            xyzrgb[:, 0:3] -= np.amin(xyzrgb, axis=0)[0:3]\n",
    "\n",
    "            filename_labels = os.path.join(folder, dataset, 'label.npy')\n",
    "            print(f'{datetime.now()}-Loading {filename_labels}...')\n",
    "            labels = np.load(filename_labels).astype(int).flatten()\n",
    "\n",
    "            xyz, rgb = np.split(xyzrgb, [3], axis=-1)\n",
    "            xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "            xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "            xyz_center = (xyz_min + xyz_max) / 2\n",
    "            xyz_center[0][-1] = xyz_min[0][-1]\n",
    "            # Remark: Don't do global alignment.\n",
    "            # xyz = xyz - xyz_center\n",
    "            rgb = rgb / 255.0\n",
    "            max_room_x = np.max(xyz[:, 0])\n",
    "            max_room_y = np.max(xyz[:, 1])\n",
    "            max_room_z = np.max(xyz[:, 2])\n",
    "            \n",
    "            offsets = [('zero', 0.0), ('half', args.block_size / 2)]\n",
    "            for offset_name, offset in offsets:\n",
    "                idx_h5 = 0\n",
    "                idx = 0\n",
    "\n",
    "                print(f'{datetime.now()}-Computing block id of {xyzrgb.shape[0]} points...')\n",
    "                xyz_min = np.amin(xyz, axis=0, keepdims=True) - offset\n",
    "                xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "                block_size = (args.block_size, args.block_size, 2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "                # Note: Don't split over z axis.\n",
    "                xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "\n",
    "                print(f'{datetime.now()}-Collecting points belong to each block...')\n",
    "                blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                            return_counts=True, axis=0)\n",
    "                block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "                print(f'{datetime.now()}-{dataset} is split into {blocks.shape[0]} blocks.')\n",
    "\n",
    "                block_to_block_idx_map = dict()      \n",
    "                for block_idx in range(blocks.shape[0]):\n",
    "                    block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "                    block_to_block_idx_map[(block[0], block[1])] = block_idx                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 06:41:30.125313-C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn\\Area_1/conferenceRoom_1 already processed, skipping\n",
      "2020-06-09 06:41:30.125313-C:/Users/SING1761/OneDrive Corp/OneDrive - Atkins Ltd/my_work/3D_data/jan_pointnet/data/pvcnn\\Area_1/conferenceRoom_2 already processed, skipping\n",
      "2020-06-09 06:41:30.125313-Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for area_idx in range(1, 2):\n",
    "        folder = os.path.join(root, 'Area_%d' % area_idx)\n",
    "        datasets = [dataset for dataset in os.listdir(folder)]\n",
    "        for dataset_idx, dataset in enumerate(datasets):\n",
    "            dataset_marker = os.path.join(folder, dataset, '.dataset')\n",
    "            if os.path.exists(dataset_marker):\n",
    "                print(f'{datetime.now()}-{folder}/{dataset} already processed, skipping')\n",
    "                continue\n",
    "            filename_data = os.path.join(folder, dataset, 'xyzrgb.npy')\n",
    "            print(f'{datetime.now()}-Loading {filename_data}...')\n",
    "            # Modified according to PointNet convensions.\n",
    "            xyzrgb = np.load(filename_data)\n",
    "            xyzrgb[:, 0:3] -= np.amin(xyzrgb, axis=0)[0:3]\n",
    "\n",
    "            filename_labels = os.path.join(folder, dataset, 'label.npy')\n",
    "            print(f'{datetime.now()}-Loading {filename_labels}...')\n",
    "            labels = np.load(filename_labels).astype(int).flatten()\n",
    "\n",
    "            xyz, rgb = np.split(xyzrgb, [3], axis=-1)\n",
    "            xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "            xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "            xyz_center = (xyz_min + xyz_max) / 2\n",
    "            xyz_center[0][-1] = xyz_min[0][-1]\n",
    "            # Remark: Don't do global alignment.\n",
    "            # xyz = xyz - xyz_center\n",
    "            rgb = rgb / 255.0\n",
    "            max_room_x = np.max(xyz[:, 0])\n",
    "            max_room_y = np.max(xyz[:, 1])\n",
    "            max_room_z = np.max(xyz[:, 2])\n",
    "\n",
    "            offsets = [('zero', 0.0), ('half', args.block_size / 2)]\n",
    "            for offset_name, offset in offsets:\n",
    "                idx_h5 = 0\n",
    "                idx = 0\n",
    "\n",
    "                print(f'{datetime.now()}-Computing block id of {xyzrgb.shape[0]} points...')\n",
    "                xyz_min = np.amin(xyz, axis=0, keepdims=True) - offset\n",
    "                xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "                block_size = (args.block_size, args.block_size, 2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "                # Note: Don't split over z axis.\n",
    "                xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "\n",
    "                print(f'{datetime.now()}-Collecting points belong to each block...')\n",
    "                blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                            return_counts=True, axis=0)\n",
    "                block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "                print(f'{datetime.now()}-{dataset} is split into {blocks.shape[0]} blocks.')\n",
    "\n",
    "                block_to_block_idx_map = dict()\n",
    "                for block_idx in range(blocks.shape[0]):\n",
    "                    block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "                    block_to_block_idx_map[(block[0], block[1])] = block_idx\n",
    "\n",
    "                # merge small blocks into one of their big neighbors\n",
    "                block_point_count_threshold = max_num_points/10\n",
    "                nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n",
    "                block_merge_count = 0\n",
    "                for block_idx in range(blocks.shape[0]):\n",
    "                    if block_point_counts[block_idx] >= block_point_count_threshold:\n",
    "                        continue\n",
    "\n",
    "                    block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "                    for x, y in nbr_block_offsets:\n",
    "                        nbr_block = (block[0] + x, block[1] + y)\n",
    "                        if nbr_block not in block_to_block_idx_map:\n",
    "                            continue\n",
    "\n",
    "                        nbr_block_idx = block_to_block_idx_map[nbr_block]\n",
    "                        if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n",
    "                            continue\n",
    "\n",
    "                        block_point_indices[nbr_block_idx] = np.concatenate(\n",
    "                            [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n",
    "                        block_point_indices[block_idx] = np.array([], dtype=np.int)\n",
    "                        block_merge_count = block_merge_count + 1\n",
    "                        break\n",
    "                print(f'{datetime.now()}-{block_merge_count} of {blocks.shape[0]} blocks are merged.')\n",
    "\n",
    "                idx_last_non_empty_block = 0\n",
    "                for block_idx in reversed(range(blocks.shape[0])):\n",
    "                    if block_point_indices[block_idx].shape[0] != 0:\n",
    "                        idx_last_non_empty_block = block_idx\n",
    "                        break\n",
    "\n",
    "                # uniformly sample each block\n",
    "                for block_idx in range(idx_last_non_empty_block + 1):\n",
    "                    point_indices = block_point_indices[block_idx]\n",
    "                    if point_indices.shape[0] == 0:\n",
    "                        continue\n",
    "                    block_points = xyz[point_indices]\n",
    "                    block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "                    xyz_grids = np.floor((block_points - block_min) / args.grid_size).astype(np.int)\n",
    "                    grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n",
    "                                                                             return_counts=True, axis=0)\n",
    "                    grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n",
    "                    grid_point_count_avg = int(np.average(grid_point_counts))\n",
    "                    point_indices_repeated = []\n",
    "                    for grid_idx in range(grids.shape[0]):\n",
    "                        point_indices_in_block = grid_point_indices[grid_idx]\n",
    "                        repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n",
    "                        if repeat_num > 1:\n",
    "                            point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n",
    "                            np.random.shuffle(point_indices_in_block)\n",
    "                            point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n",
    "                        point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n",
    "                    block_point_indices[block_idx] = np.array(point_indices_repeated)\n",
    "                    block_point_counts[block_idx] = len(point_indices_repeated)\n",
    "\n",
    "                for block_idx in range(idx_last_non_empty_block + 1):\n",
    "                    point_indices = block_point_indices[block_idx]\n",
    "                    if point_indices.shape[0] == 0:\n",
    "                        continue\n",
    "\n",
    "                    block_point_num = point_indices.shape[0]\n",
    "                    block_split_num = int(math.ceil(block_point_num * 1.0 / max_num_points))\n",
    "                    point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n",
    "                    point_nums = [point_num_avg] * block_split_num\n",
    "                    point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n",
    "                    starts = [0] + list(np.cumsum(point_nums))\n",
    "\n",
    "                    # Modified following convensions of PointNet.\n",
    "                    np.random.shuffle(point_indices)\n",
    "                    block_points = xyz[point_indices]\n",
    "                    block_rgb = rgb[point_indices]\n",
    "                    block_labels = labels[point_indices]\n",
    "                    x, y, z = np.split(block_points, (1, 2), axis=-1)\n",
    "                    norm_x = x / max_room_x\n",
    "                    norm_y = y / max_room_y\n",
    "                    norm_z = z / max_room_z\n",
    "                    \n",
    "                    minx = np.min(x)\n",
    "                    miny = np.min(y)\n",
    "                    x = x - (minx + args.block_size / 2)\n",
    "                    y = y - (miny + args.block_size / 2)\n",
    "                    \n",
    "                    block_xyzrgb = np.concatenate([x, y, z, block_rgb, norm_x, norm_y, norm_z], axis=-1)\n",
    "                    for block_split_idx in range(block_split_num):\n",
    "                        start = starts[block_split_idx]\n",
    "                        point_num = point_nums[block_split_idx]\n",
    "                        end = start + point_num\n",
    "                        idx_in_batch = idx % batch_size\n",
    "                        data[idx_in_batch, 0:point_num, ...] = block_xyzrgb[start:end, :]\n",
    "                        data_num[idx_in_batch] = point_num\n",
    "                        label[idx_in_batch] = dataset_idx  # won't be used...\n",
    "                        label_seg[idx_in_batch, 0:point_num] = block_labels[start:end]\n",
    "                        indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n",
    "\n",
    "                        if ((idx + 1) % batch_size == 0) or \\\n",
    "                                (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1):\n",
    "                            item_num = idx_in_batch + 1\n",
    "                            filename_h5 = os.path.join(folder, dataset, f'{offset_name}_{idx_h5:d}.h5')\n",
    "                            print(f'{datetime.now()}-Saving {filename_h5}...')\n",
    "\n",
    "                            file = h5py.File(filename_h5, 'w')\n",
    "                            file.create_dataset('data', data=data[0:item_num, ...])\n",
    "                            file.create_dataset('data_num', data=data_num[0:item_num, ...])\n",
    "                            file.create_dataset('label', data=label[0:item_num, ...])\n",
    "                            file.create_dataset('label_seg', data=label_seg[0:item_num, ...])\n",
    "                            file.create_dataset('indices_split_to_full', data=indices_split_to_full[0:item_num, ...])\n",
    "                            file.close()\n",
    "\n",
    "                            if args.save_ply:\n",
    "                                print(f'{datetime.now()}-Saving ply of {filename_h5}...')\n",
    "                                filepath_label_ply = os.path.join(folder, dataset, 'ply_label',\n",
    "                                                                  f'label_{offset_name}_{idx_h5:d}')\n",
    "                                save_ply_property_batch(data[0:item_num, :, 0:3], label_seg[0:item_num, ...],\n",
    "                                                        filepath_label_ply, data_num[0:item_num, ...], 14)\n",
    "\n",
    "                                filepath_rgb_ply = os.path.join(folder, dataset, 'ply_rgb',\n",
    "                                                                f'rgb_{offset_name}_{idx_h5:d}')\n",
    "                                save_ply_color_batch(data[0:item_num, :, 0:3], data[0:item_num, :, 3:6] * 255,\n",
    "                                                     filepath_rgb_ply, data_num[0:item_num, ...])\n",
    "\n",
    "                            idx_h5 = idx_h5 + 1\n",
    "                        idx = idx + 1\n",
    "\n",
    "            # Marker indicating we've processed this dataset\n",
    "            open(dataset_marker, 'w').close()\n",
    "    print(f'{datetime.now()}-Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
